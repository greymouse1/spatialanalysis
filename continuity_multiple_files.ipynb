{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "https://github.com/greymouse1/spatialanalysis/blob/main/continuity_multiple_files.ipynb",
      "authorship_tag": "ABX9TyNtu1WjujfSWEOowdOaAGl9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/greymouse1/spatialanalysis/blob/main/continuity_multiple_files.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Code is licenced under MIT licence.\n",
        "\n",
        "Author: Nikola G.\n",
        "\n",
        "Credits:\n",
        "\n",
        "Tutorial from Momepy package website at http://docs.momepy.org/en/stable/user_guide/graph/coins.html\n",
        "based on paper by Tripathy et al. (2020)\n",
        "\n",
        "OpenAI. (2024). ChatGPT (version 4) [Large language model]. OpenAI. https://openai.com/chatgpt\n",
        "\n",
        "\n",
        "\n",
        "Tripathy, P., Rao, P., Balakrishnan, K., & Malladi, T. (2020). An open-source tool to extract natural continuity and hierarchy of urban street networks. Environment and Planning B: Urban Analytics and City Science. http://dx.doi.org/10.1177/2399808320967680"
      ],
      "metadata": {
        "id": "D9-uae52ZshA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Gs0eOwbHEqN"
      },
      "outputs": [],
      "source": [
        "!pip install osmnx > /dev/null 2>&1\n",
        "!pip install momepy > /dev/null 2>&1\n",
        "!pip install mapclassify>=2.4.0 > /dev/null 2>&1 # install mapclassify with version >=2.4.0\n",
        "!pip install powerlaw > /dev/null 2>&1\n",
        "import osmnx as ox\n",
        "import geopandas as gpd\n",
        "import momepy\n",
        "import mapclassify\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import os\n",
        "import numpy as np\n",
        "import powerlaw\n",
        "from collections import defaultdict, Counter\n",
        "from shapely.geometry import MultiLineString, LineString, Point"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def naturalCities(currentCity,folderPath,cityName):\n",
        "  # Retrieve the graph within the polygon's boundaries\n",
        "  # This will pull OSM data from inside the polygon and create a networkX graph\n",
        "\n",
        "  graph = ox.graph_from_polygon(\n",
        "      currentCity,\n",
        "      network_type='drive',  # Choose network type (e.g., 'drive', 'walk', 'bike', etc.)\n",
        "      simplify=True,         # Simplify graph (remove unnecessary nodes)\n",
        "      retain_all=False,      # Keep only the largest connected component\n",
        "      truncate_by_edge=False  # Truncate by edge to keep nodes near the edge\n",
        "  )\n",
        "\n",
        "  # Reproject graph\n",
        "  # Choice of final projection is automatic, original must be WGS84\n",
        "\n",
        "  city_streets = ox.projection.project_graph(graph)\n",
        "\n",
        "  # Create gdf from graph so it can be used later on\n",
        "\n",
        "  city_gdf = ox.graph_to_gdfs(\n",
        "      ox.convert.to_undirected(city_streets),\n",
        "      nodes=False,\n",
        "      edges=True,\n",
        "      node_geometry=False,\n",
        "      fill_edge_geometry=True,\n",
        "  )\n",
        "\n",
        "  # Calculate continuity from the gdf\n",
        "\n",
        "  continuity = momepy.COINS(city_gdf, angle_threshold=135, flow_mode=False)\n",
        "\n",
        "  # Pull out stroke\n",
        "\n",
        "  city_stroke_gdf = continuity.stroke_gdf()\n",
        "\n",
        "  # Save stroke to .shp\n",
        "  shapefile_path = os.path.join(folderPath, f\"polygon_{cityName}.shp\")\n",
        "  city_stroke_gdf.to_file(shapefile_path)\n",
        "#--------------------------------------------------------------------------------------------------------------\n",
        "  # Initialize the vertex-to-linestring mapping\n",
        "  vertex_to_linestring = defaultdict(list)\n",
        "\n",
        "  # Iterate over geometries and map vertices to LineStrings (handle MultiLineString)\n",
        "  for idx, geom in city_stroke_gdf.geometry.items():\n",
        "      if isinstance(geom, LineString):  # Process single LineString\n",
        "          for point in geom.coords:\n",
        "              vertex_to_linestring[Point(point)].append(idx)\n",
        "      elif isinstance(geom, MultiLineString):  # Process MultiLineString\n",
        "          # For each LineString in MultiLineString\n",
        "          for subline in geom.geoms:\n",
        "              for point in subline.coords:\n",
        "                  vertex_to_linestring[Point(point)].append(idx)\n",
        "\n",
        "  # Initialize a dictionary to store the connection counts for each line\n",
        "  line_connections = defaultdict(int)\n",
        "\n",
        "  # Iterate over the geometries again to count connections\n",
        "  for idx, geom in city_stroke_gdf.geometry.items():\n",
        "      if isinstance(geom, LineString):  # Process single LineString\n",
        "          connections = []  # Use a list to count multiple connections\n",
        "          for point in geom.coords:\n",
        "              for connected_line in vertex_to_linestring[Point(point)]:\n",
        "                  if connected_line != idx:\n",
        "                      connections.append(connected_line)  # Add connection to the list\n",
        "          line_connections[idx] = len(connections)  # Store the total number of connections\n",
        "      elif isinstance(geom, MultiLineString):  # Process MultiLineString\n",
        "          # For each LineString in MultiLineString\n",
        "          for subline in geom.geoms:\n",
        "              connections = []  # Use a list to count multiple connections for each subline\n",
        "              for point in subline.coords:\n",
        "                  for connected_line in vertex_to_linestring[Point(point)]:\n",
        "                      if connected_line != idx:\n",
        "                          connections.append(connected_line)  # Add connection to the list\n",
        "              # Store the connection count for each subline (if necessary)\n",
        "              # For now, we store the count for the entire MultiLineString\n",
        "              line_connections[idx] = len(connections)\n",
        "\n",
        "\n",
        "  # Extract degree (number of connections) values\n",
        "  degree_values = list(line_connections.values())\n",
        "\n",
        "  # Count frequencies of degree values\n",
        "  degree_counts = Counter(degree_values)\n",
        "\n",
        "  # Extract x (degrees) and y (frequencies)\n",
        "  x = np.array(list(degree_counts.keys()))       # Unique degree values\n",
        "  y = np.array(list(degree_counts.values()))    # Frequency of each degree\n",
        "\n",
        "  # Fit the degree distribution to a power-law model\n",
        "  fit = powerlaw.Fit(degree_values)\n",
        "  print(f\"Alpha: {fit.alpha}\")\n",
        "  print(f\"xmin: {fit.xmin}\")\n",
        "\n",
        "  # Get alpha and xmin (scaling parameter and lower bound for the power-law fit)\n",
        "  alpha = fit.alpha\n",
        "  xmin = fit.xmin\n",
        "\n",
        "  # Get the p-value from the goodness-of-fit test\n",
        "  p_value = fit.power_law.KS()\n",
        "\n",
        "  print(f\"p-value: {p_value}\")\n",
        "  print(\"-----------------------\")\n",
        "  # Compare the power-law fit with an alternative distribution (e.g., exponential)\n",
        "  R, p_alt = fit.distribution_compare('power_law', 'exponential')\n",
        "  print(f\"Log-likelihood ratio (R): {R}\")\n",
        "  print(f\"p-value for comparison: {p_alt}\")\n",
        "\n",
        "  # Plot the data and the fitted power law\n",
        "  fig = fit.plot_pdf(marker='o', color='blue', markersize=4, label='Empirical Data')\n",
        "  fit.power_law.plot_pdf(ax=fig, color='red', linestyle='--', linewidth=1, label='Power Law Fit')\n",
        "\n",
        "  # Add legend and labels\n",
        "  plt.xlabel(\"Degree (Connections)\")\n",
        "  plt.ylabel(\"Frequency\")\n",
        "  plt.legend()\n",
        "\n",
        "  output_path_power = os.path.join(folderPath, f\"power_{cityName}.png\")\n",
        "  plt.savefig(output_path_power, dpi=600, bbox_inches=\"tight\")\n",
        "\n",
        "  print(f\"Plot for power fit for {cityName} saved to {output_path_power}\")\n",
        "  plt.clf() # clear existing plot and make space for a new one\n",
        "\n",
        "#-------------------------\n",
        "  # Add connections to original gdf\n",
        "  city_stroke_gdf['n_connections'] = degree_values\n",
        "\n",
        "  # Calculate heavy tailed classification\n",
        "  classifier = mapclassify.HeadTailBreaks(degree_values)\n",
        "\n",
        "  # Get classification details\n",
        "  class_intervals = classifier.bins  # Class boundaries\n",
        "  counts = classifier.counts         # Number of features in each class\n",
        "  labels = classifier.yb             # Class labels for each feature\n",
        "\n",
        "  # Save classification output to a text file\n",
        "  classifier_path = os.path.join(folderPath, f\"map_classifier_output_{cityName}.txt\")\n",
        "\n",
        "  with open(classifier_path, \"w\") as file:\n",
        "      file.write(\"HeadTailBreaks Classification\\n\")\n",
        "      file.write(\"================================\\n\")\n",
        "      file.write(\"Total segments:\\n\")\n",
        "      file.write(f\"{len(degree_values)}\" + \"\\n\\n\")\n",
        "      file.write(\"HT index of street connectivity:\\n\")\n",
        "      file.write(f\"{len(class_intervals)}\" + \"\\n\\n\")\n",
        "      file.write(\"Class Intervals:\\n\")\n",
        "      file.write(\", \".join(f\"{b:.2f}\" for b in class_intervals) + \"\\n\\n\")\n",
        "      file.write(\"Counts in Each Class:\\n\")\n",
        "      file.write(\", \".join(str(c) for c in counts) + \"\\n\\n\")\n",
        "      file.write(\"Statistical indicators for power law:\" + \"\\n\\n\")\n",
        "      file.write(\"Alpha: \" + str(alpha) + \"\\n\")\n",
        "      file.write(\"xmin: \" + str(xmin) + \"\\n\")\n",
        "      file.write(\"p-value: \" + str(p_value) + \"\\n\")\n",
        "      file.write(\"Log-likelihood ratio (R): \" + str(R) + \"\\n\")\n",
        "      file.write(\"p-value for comparison: \" + str(p_alt) + \"\\n\")\n",
        "\n",
        "\n",
        "  print(f\"Classifier output saved to {classifier_path}\")\n",
        "\n",
        "  # Show the classifier (lower and upper bounds plus count)\n",
        "  print(classifier)\n",
        "\n",
        "  fig, ax = plt.subplots(figsize=(15, 15))\n",
        "\n",
        "  city_stroke_gdf.plot(\n",
        "      cmap=\"gist_rainbow\",\n",
        "      column=\"n_connections\",\n",
        "      legend=True,\n",
        "      scheme=\"headtailbreaks\",\n",
        "      linewidth=0.5,\n",
        "      ax=ax,\n",
        "  ).set_axis_off()\n",
        "\n",
        "  fig.patch.set_facecolor(\"black\")\n",
        "\n",
        "  output_path_figure = os.path.join(folderPath, f\"figure_{cityName}.png\")\n",
        "  plt.savefig(output_path_figure, dpi=600, bbox_inches=\"tight\")\n",
        "  plt.close(fig)\n",
        "  print(f\"Plot for {cityName} saved to {output_path_figure}\")\n",
        "\n",
        "  # Histogram for n_segments\n",
        "\n",
        "  city_stroke_gdf['n_connections'].plot(kind='hist', bins=40, title='n_segments')\n",
        "  plt.gca().spines[['top', 'right',]].set_visible(False)\n",
        "  output_path_histogram = os.path.join(folderPath, f\"histogram_{cityName}.png\")\n",
        "  plt.savefig(output_path_histogram, dpi=300, bbox_inches=\"tight\")\n",
        "  print(f\"Histogram for {cityName} saved to {output_path_histogram}\")\n",
        "  plt.clf() # clear existing plot and make space for a new one"
      ],
      "metadata": {
        "id": "3PHRo9z2-q_m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Load shp file with all the polygons\n",
        "all_pol = gpd.read_file(\"/content/drive/MyDrive/spatialanalysis/all_pol_names/all_pol_names.shp\")\n",
        "\n",
        "base_folder = \"output_polygons\"\n",
        "os.makedirs(base_folder, exist_ok=True)  # Create base folder if it doesn't exist\n",
        "\n",
        "# Iterate through each polygon and create a new GeoDataFrame\n",
        "for index, row in all_pol.iterrows():\n",
        "\n",
        "    # Create a new GeoDataFrame for the current polygon\n",
        "    new_gdf = gpd.GeoDataFrame(\n",
        "        [row],\n",
        "        columns=all_pol.columns,\n",
        "        crs=all_pol.crs  # Retain the original CRS\n",
        "    )\n",
        "    polygon = new_gdf.geometry.iloc[0]  # Extract the Polygon/MultiPolygon geometry\n",
        "    city_name = row['name']  # Extract the city name from the row\n",
        "\n",
        "    # Define the folder name for the current polygon\n",
        "    polygon_folder = os.path.join(base_folder, f\"{city_name}\")\n",
        "\n",
        "    # Save or process the new GeoDataFrame\n",
        "    print(f\"New GeoDataFrame for {city_name} is loaded\")\n",
        "    if city_name == \"Jakarta\":\n",
        "      next\n",
        "    else:\n",
        "      os.makedirs(polygon_folder, exist_ok=True)  # Create folder if it doesn't exist\n",
        "      naturalCities(polygon,polygon_folder,city_name)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "JuCM8eBTNlhO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Zip files for download\n",
        "\n",
        "!zip -r /content/output_polygons.zip /content/output_polygons"
      ],
      "metadata": {
        "id": "asD-ez-uRg0Z"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}